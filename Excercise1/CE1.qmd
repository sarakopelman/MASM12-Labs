---
title: "CE1"
format: html
---
## Computer Exercise 1 - Markus Gerholm, Sara Kopelman, Erik Goransson Gaspar


## Part 1

### SETAR(2;1;1)

```{r}
rm(list=ls())
library(rgl)

n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]
for(t in 2:n)
  {
    if(y[t-1] > 0)
      {
        y[t] <- 0 + 0.8 * y[t-1] + r[t]
      }
    else
      {
        y[t] <- 0 - 0.8 * y[t-1] + r[t]
      }
  }
## Put it into a data.frame, and make x1 and y1 which are lagged one step 

plot(1:n, y, type = "l")
```

### IGAR(2;1)

```{r}
n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]
for(t in 2:n)
  {
    if(rbinom(1,1,0.3) == 1)
      {
        y[t] <- 3 + 0.8 * y[t-1] + r[t]
      }
    else
      {
        y[t] <- -3 - 0.8 * y[t-1] + r[t]
      }
  }
## Put it into a data.frame, and make x1 and y1 which are lagged one step 
##------------------------------------------------

plot(1:n, y, type = "l")

```

### MMAR(2;1)

```{r}
n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]

P = matrix(c(0.95, 0.05, 0.95, 0.05), nrow = 2, ncol = 2, byrow = TRUE)
current = 1
for(t in 2:n)
  {
    if(rbinom(1,1, P[current,1])==1 )
      {
        y[t] <- 3 + 0.8 * y[t-1] + r[t]
        current=1
      }
    else
      {
        y[t] <- -3 - 0.8 * y[t-1] + r[t]
        current=2
      }
  }
## Put it into a data.frame, and make x1 and y1 which are lagged one step 
##------------------------------------------------

plot(1:n, y, type = "l")

```

## Part 2

### Theoretical conditional mean for SETAR(2;1;1)

```{r}
n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]
for(t in 2:n)
  {
    if(y[t-1] < 0)
      {
        y[t] <- 3 + 0.8 * y[t-1] + r[t]
      }
    else
      {
        y[t] <- -3 - 0.8 * y[t-1] + r[t]
      }
}
M <- rep(NA, n-1)
for (t in 2:n) {
  if (y[t-1] < 0) {
    M[t-1] <- 3 + 0.8 * y[t-1]
  } else {
    M[t-1] <- -3 - 0.8 * y[t-1]
  }
}

## Put it into a data.frame, and make x1 and y1 which are lagged one step 

plot(3:202, M[1:200], type = "l")
lines(2:201, y[1:200], col = "red")
```

### Estimating conditional mean
```{r}

D <- data.frame(y=y[-1], y1=y[-n])

fit <- loess(y ~ y1, dat=D, span=0.2)
yprd_2d <- predict(fit, data.frame(y1=D$y1))

plot(y[2:n], yprd_2d)

#plot(1:length(yprd_2d), yprd_2d, type = "l")
#lines(1:length(yprd_2d), yprd_2d, col = "red")

```

## Part 3

### Theoretical cumulative conditional mean
```{r}
n <- 1:20
theoretical_lambda <- numeric(length(n))

for (i in 1:10)  theoretical_lambda[i]  <-  0.16 * n[i]
for (i in 11:20) theoretical_lambda[i]  <-  0.16 * n[i]

cum_lambda <- cumsum(theoretical_lambda)
x <- seq(-2, 2, length.out = 20)
plot(x, theoretical_lambda, type = "l")
```



### Estimated cumulative conditional mean
```{r}
## Script for calculation of a confidence bands using the cumulative means technique.
## First generate x: a time series with a realization of a Markov process, i.e. some
## model where x_t is dependent on x_{t-1}

## Parameters for the histogram regression
## Number of intervals 
n.bin <- 20
## The breaks between the intervals 
breaks <- seq(-2,2,len=n.bin+1)
## Initialize
h <- diff(breaks)[1]
lambda <- gamma <- f.hat <- h.hat <- numeric(n.bin)

## Cut into intervals conditioned on x_{t-1}
L <- split(y[-1], cut(y[-length(y)],breaks))
## Check if there are at least 5 points in each interval
if(!all(sapply(L,length)>=5)){ print('Stopped: There are less than 5 points in one of the intervals'); break;}
## Calc the hist regressogram, i.e. for each interval
for(i in 1:n.bin)
  {
    y.bin <- L[[i]]
    lambda[i] <- mean(y.bin)
    f.hat[i] <- (n.bin*h)^(-1) * length(y.bin)
    gamma[i] <- sum((y.bin - lambda[i])^2) / length(y.bin)
  }
## Make confidence bands for the cumulated function. Def. (3.10).
## 95% confidence band, c is found in table 3.1
c.alpha <- 1.273
##
Lambda <- cumsum(lambda*h)
for(i in 1:n.bin)
  {
    h.hat[i] <- gamma[i]/f.hat[i];
  }
H.hat <- cumsum(h.hat*h);
##
H.hat.b <- H.hat[n.bin];
Lambda.lower <- Lambda - c.alpha * n.bin^(-0.5) * H.hat.b^(0.5) * (1 + H.hat/H.hat.b);
Lambda.upper <- Lambda + c.alpha * n.bin^(-0.5) * H.hat.b^(0.5) * (1 + H.hat/H.hat.b);

plot(Lambda, type = "l")

```

## Part 4

### Loading data

```{r}
library(readr)
DataPart4 <- read_csv("/Users/markusgerholm/Desktop/Non-linear time series/MASM12-Labs/CE1/Data/DataPart4.csv")
```

```{r}

Ph <- DataPart4$Ph
Ti <- DataPart4$Ti
Te <- DataPart4$Te
W <- DataPart4$W

U <- Ph/(Ti-Te)
DataPart4$U <- U
# Plotting W against U
# There is outlier with negative heat loss coefficient, comment and remove since it should be non-zero (double check)
plot(W,U)

# Fitting local polynomial regression
fit <- loess(U ~ W, data = DataPart4, span = 0.5, degree = 2)

v <- seq(0, 10, length.out = 1000)
U_prd <- predict(fit, v)
plot(v, U_prd, type='l')
```
### Checking residuals of fitted model

```{r}
qqnorm(fit$residuals)
qqline(fit$residuals, col = "red")
```

## Part 5

### Loading data
```{r}
DataPart5 <- read_csv("/Users/markusgerholm/Desktop/Non-linear time series/MASM12-Labs/Excercise1/DataPart5.csv")
```

### Modelling ARMA

```{r}
x <- DataPart5$x  
x <- x-mean(x)
plot(x, type = "l")
acf(x, lag.max = 40, main = "ACF of series")
pacf(x, lag.max = 40, main = "PACF of series")
```
### Adding p = 1 on t-2 based on PACF

```{r}
fit <- arima(x, order = c(2, 0, 0),
             include.mean = FALSE,
             fixed = c(0, NA),          # AR1 = 0, AR2 = estimate
             transform.pars = FALSE)

res <- residuals(fit)

#par(mfrow = c(1,2))
acf(res, lag.max = 40, main = "ACF of residuals")
pacf(res, lag.max = 40, main = "PACF of residuals")
par(mfrow = c(1,1))

# 95% reference band for ACF/PACF (rough):
n <- length(res)
ci <- qnorm(0.975)/sqrt(n)
abline(h = c(-ci, ci), lty = 2)  # add after each plot if you like

# Ljungâ€“Box test: want a large p-value (no remaining autocorrelation)
Box.test(res, lag = 24, type = "Ljung", fitdf = 2)
```

### LDF

```{r}
leaveOneOut <- function(D,plotIt=FALSE)
  {
    ## Find the bandwidth giving the best balance between bias and variance
    ## of the model by leave one out.

    ## Plot the data
    if(plotIt)
      {
        dev.new()
        par(mfrow=c(1,2))
        plot(D$xk, D$x)
      }
    ## Make the vector of bandwidths which are fitted
    span <- c(seq(0.2, 1, by=0.1),2,4,10)
    ## Matrix for keeping the residuals
    R <- matrix(nrow=nrow(D), ncol=length(span))
    ## Do it for each bandwidth
    for(ii in 1:length(span))
      {
        print(paste("  Fitting for bandwidth",ii,"of",length(span)))
        ## Do the local 2-order polynomial regression one time for each point
        ## leaving the point out while fitting, and then predicting the point.
        for(i in 1:nrow(D))
          {
            R[i,ii] <- D[i,"x"] - predict(loess(x ~ xk, dat=D[-i,], span=span[ii]), D[i,])
          }
      }
    ## Find the best bandwidth
    RSSkAll <- apply(R, 2, function(x){sum(x^2,na.rm=TRUE)})
    ## Take the RRS for the best span value
    spanBest <- span[which.min(RSSkAll)]
    ## Calculate the RSSk
    RSSk <- sum((D$x - predict(loess(x ~ xk, dat=D, span=spanBest), D))^2,na.rm=TRUE)
    ## Plot the fitted function
    if(plotIt)
      {
        DT <- D[order(D$xk),]
        lines(DT$xk, predict(loess(x ~ xk, dat=D, span=spanBest), DT), col=2)
        ## Plot RSSk as a function of the bandwidth
        plot(span, RSSkAll)
      }
    ##
    return(RSSk)
  }

## Make a simple ldf(x,lags)
ldf <- function(x,lags,nBoot=30,plotIt=TRUE,plotFits=FALSE)
{  
  ## Calculate Lag Dependence Functions
  ## by local 2-order polynomial regression with the loess() function,
  ## and leave one out to find the best bandwidth. Finally an approximate
  ## 95% confidence interval is calculated with simple bootstrapping.
  ## Input:
  ## x, is the time series to be analysed
  ## lags, are the values for which the LDF is calculated
  ## nBoot, is the number of bootstrapping samples to make
  ## plotIt, should the ldf be plotted
  ## plotFits, should the smoothed be plotted

  ## The result is kept in val
  val <- vector()
  ##
  for(i in 1:length(lags))
    {
      ## Take the k
      k <- lags[i]
      ## print text
      print(paste("Calculating ldf no. ",i," of ",length(lags), sep=""))
      ## Dataframe for modelling: xk is lagged k steps
      D <- data.frame(x=x[-(1:k)],xk=x[-((length(x)-k+1):length(x))])
      ## Leave one out optimization of the bandwidth with loess
      RSSk <- leaveOneOut(D,plotFits)
      ## Calculate the ldf
      RSS <- sum((D$x - mean(D$x))^2)
      val[i] <- (RSS - RSSk) / RSS
    }      
  
  ## Very simple bootstrapping
  iidVal <- vector()
  for(i in 1:nBoot)
    {
      ## Print to entertain the modeller ;-)
      print(paste("Calculating bootstrap no. ",i," of ",nBoot, sep=""))
      ## Bootstrapping to make a confidence band
      xr <- sample(x, min(length(x),100) ,replace=TRUE)
      ## Dataframe for modelling
      DR <- data.frame(x=xr[-1],xk=xr[-length(xr)])
      RSSk <- leaveOneOut(DR)
      ## The ldf is then calculated
      RSS <- sum((DR$x - mean(DR$x))^2)
      (iidVal[i] <- (RSS - RSSk) / RSS)
    }

  ## Plot the ldf
  if(plotIt)
    {
      dev.new()
      plot(c(0,lags), c(1,val), type="n", ylim=c(-1,1), ylab="LDF", main="Lag Dependence Functions", xaxt="n", xlab="lag")
      axis(1,c(0,lags))
      abline(0,0,col="gray")
      lines(c(0,lags), c(1,val), type="h")
      ## Draw the approximate 95% confidence interval
      abline(h=quantile(iidVal,0.95), col="blue", lty=2)
    }
  return(val)
}
```

### Plotting lag dependence function

```{r}
val <- ldf(res, 1:10)

# LDF significant at lag 2!!!
```

### Finding reasonable regions

```{r}
n <- length(x)
plot(x[3:n], x[1:(n-2)]) # X_n vs X_{n-2}
abline(h=c(-4.2, -0.23, 2.7)) # Reasonable cut-offs for the regions
```
### Modelling the regimes by SETAR










