---
title: "CE1"
format: html
---
## Computer Exercise 1 - Markus Gerholm, Sara Kopelman, Erik Goransson Gaspar


## Part 1

### SETAR(2;1;1)

```{r}
rm(list=ls())
library(rgl)

n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]
for(t in 2:n)
  {
    if(y[t-1] > 0)
      {
        y[t] <- 0 + 0.8 * y[t-1] + r[t]
      }
    else
      {
        y[t] <- 0 - 0.8 * y[t-1] + r[t]
      }
  }
## Put it into a data.frame, and make x1 and y1 which are lagged one step 

plot(1:n, y, type = "l")

plot(head(y, -1), tail(y, -1))
```

### IGAR(2;1)

```{r}
n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]
for(t in 2:n)
  {
    if(rbinom(1,1,0.3) == 1)
      {
        y[t] <- 3 + 0.8 * y[t-1] + r[t]
      }
    else
      {
        y[t] <- -3 - 0.8 * y[t-1] + r[t]
      }
  }
## Put it into a data.frame, and make x1 and y1 which are lagged one step 
##------------------------------------------------

plot(1:n, y, type = "l")

```

### MMAR(2;1)

```{r}
n <- 1000
## Uniform distributed x
## Errors
r <- rnorm(n)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]

P = matrix(c(0.95, 0.05, 0.95, 0.05), nrow = 2, ncol = 2, byrow = TRUE)
current = 1
for(t in 2:n)
  {
    if(rbinom(1,1, P[current,1])==1 )
      {
        y[t] <- 3 + 0.8 * y[t-1] + r[t]
        current=1
      }
    else
      {
        y[t] <- -3 - 0.8 * y[t-1] + r[t]
        current=2
      }
  }
## Put it into a data.frame, and make x1 and y1 which are lagged one step 
##------------------------------------------------

plot(1:n, y, type = "l")

```

## Part 2

### Theoretical conditional mean for SETAR(2;1;1)

```{r}
set.seed(5)
n <- 1000
## Errors
r <- rnorm(n,sd=1)
## Make a time series y with a regime model
y <- rep(NA,n)
y[1] <- r[1]
for(t in 2:n)
  {
    if(y[t-1] < 0)
      {
        y[t] <- 3 + 0.8 * y[t-1] + r[t]
      }
    else
      {
        y[t] <- -3 - 0.8 * y[t-1] + r[t]
      }
}

#Plot theoretical conditional mean:
# --- Conditional mean function ---
cond_mean <- function(y_prev) {
  ifelse(y_prev < 0, 3 + 0.8 * y_prev, -3 - 0.8 * y_prev)
}

# --- Range for plotting ---
y_prev_seq <- seq(-10, 10, length.out = 400)

# --- Split into two regimes ---
y_neg <- y_prev_seq[y_prev_seq < 0]
y_pos <- y_prev_seq[y_prev_seq >= 0]

# --- Plot empty frame first ---
plot(y_prev_seq, cond_mean(y_prev_seq), type = "n",
     main = expression("Piecewise Conditional Mean  E[Y[t] | Y[t-1]]"),
     xlab = expression(Y[t-1]), ylab = expression(E[Y[t] ~ "|" ~ Y[t-1]]),
     ylim = c(-10, 10))
abline(h = 0, v = 0, col = "gray", lty = 2)

# --- Add simulated points ---
points(y[-n], y[-1], pch = 16, cex = 0.5, col = rgb(0, 0, 0, 0.25))

# --- Add the two theoretical segments ---
lines(y_neg, 3 + 0.8 * y_neg, col = "red", lwd = 2)
lines(y_pos, -3 - 0.8 * y_pos, col = "blue", lwd = 2)

# --- Mark threshold and legend ---
abline(v = 0, col = "gray40", lty = 3, lwd = 2)
legend("topright", legend = c("y[t-1] < 0", "y[t-1] ≥ 0"),
       col = c("red", "blue"), lwd = 2, bty = "n")


M <- rep(NA, n-1) 
for (t in 2:n) {
  if (y[t-1] < 0) {
    M[t-1] <- 3 + 0.8 * y[t-1]
  } else {
    M[t-1] <- -3 - 0.8 * y[t-1]
  }
}

## Put it into a data.frame, and make x1 and y1 which are lagged one step 

plot(3:202, M[1:200], type = "l")
lines(2:201, y[1:200], col = "red")

plot(head(y, -1), tail(y, -1))

plot(y, type = "l", col = "blue",
     xlab = "Time t", ylab = expression(X[t]),
     main = "SETAR(2,1,2) Process")
abline(h = 0, lty = 2, col = "red")  # threshold at 0
grid()

# Shorter range (first 100 points)
range_end <- 100

# 1️⃣ Line plot of M vs y
plot(3:(range_end+2), M[1:range_end], type = "l",
     xlab = "Time t", ylab = "M",
     main = "Conditional Mean vs Process")
lines(2:(range_end+1), y[1:range_end], col = "red")
legend("topright", legend = c("M", "y"), col = c("black", "red"), lty = 1, bty="n")
grid()

# 2️⃣ Lag plot
plot(head(y, range_end), tail(y, range_end),
     xlab = expression(X[t-1]), ylab = expression(X[t]),
     main = "Lag plot")
grid()

# 3️⃣ Time series plot
plot(y[1:range_end], type = "l", col = "blue",
     xlab = "Time t", ylab = expression(X[t]),
     main = "SETAR(2,1,1) Process")
abline(h = 0, lty = 2, col = "red")  # threshold at 0
grid()

#plot(head(M-y, -1), tail(M-y, -1))
```

### Estimating conditional mean
```{r}
# -------------------------
# Prepare data
# -------------------------
D <- data.frame(y = y[-1], y1 = y[-length(y)])
M_theoretical <- ifelse(D$y1 < 0, 3 + 0.8 * D$y1, -3 - 0.8 * D$y1)

# Bandwidths to try
spans <- c(0.1, 0.3, 0.5, 0.7)

# Prepare sequence for theoretical mean lines
v_left <- seq(min(D$y1), 0, length.out = 500)
v_right <- seq(0, max(D$y1), length.out = 500)
M_left <- 3 + 0.8 * v_left
M_right <- -3 - 0.8 * v_right

# -------------------------
# Precompute LOESS fits
# -------------------------
fits <- lapply(spans, function(s) {
  fit <- loess(y ~ y1, data = D, span = s)
  predict(fit, newdata = data.frame(y1 = D$y1))
})

# -------------------------
# Plot 1: Conditional Mean Functions (2x2)
# -------------------------
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))  # 2 rows, 2 columns

for(i in seq_along(spans)){
  s <- spans[i]
  M_hat <- fits[[i]]
  
  plot(D$y1, D$y,
       pch = 16, col = rgb(0.2, 0.2, 0.7, 0.4),
       xlab = expression(X[t]),
       ylab = expression(E[X[t+1] ~ "|" ~ X[t] == x]),
       main = paste("Span =", s))
  lines(D$y1[order(D$y1)], M_hat[order(D$y1)], col = "red", lwd = 2)
  lines(v_left, M_left, col = "black", lwd = 2, lty = 2)
  lines(v_right, M_right, col = "black", lwd = 2, lty = 2)
  abline(v = 0, lty = 3, col = "gray40")
  grid()
}

# -------------------------
# Plot 2: Predicted vs Theoretical (2x2)
# -------------------------
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))  # 2 rows, 2 columns

for(i in seq_along(spans)){
  s <- spans[i]
  M_hat <- fits[[i]]
  
  plot(M_theoretical, M_hat,
       pch = 16, col = rgb(0.2, 0.2, 0.7, 0.4),
       xlab = expression("Theoretical mean " ~ M(X[t])),
       ylab = expression("LOESS estimate " ~ hat(M)(X[t])),
       main = paste("Span =", s))
  abline(0, 1, col = "red", lty = 2, lwd = 2)
  grid()
}



```

## Part 3

### Theoretical cumulative conditional mean
```{r}
# -------------------------
# SETAR cumulative conditional mean
# -------------------------
a_setar <- -2                  # starting point
b_max_setar <- 2               # maximum b value
b_setar <- seq(a_setar, b_max_setar, length.out = 500)

lambda_setar <- ifelse(b_setar < 0,
                       3*(b_setar - a_setar) + 0.4*(b_setar^2 - a_setar^2),
                       ifelse(b_setar == 0,
                              3*(0 - a_setar) + 0.4*(0^2 - a_setar^2),
                              (3*(0 - a_setar) + 0.4*(0^2 - a_setar^2)) +
                                (-3*(b_setar - 0) - 0.4*(b_setar^2 - 0^2))
                       )
)

plot(b_setar, lambda_setar, type="l", lwd=2, col="blue",
     xlab="b", ylab=expression(Lambda(b)),
     main=paste0("SETAR Cumulative Lambda (a=", a_setar, ", b_max=", b_max_setar, ")"))
abline(h=0, lty=2, col="gray")
abline(v=0, lty=2, col="gray")

# -------------------------
# AR(1) cumulative conditional mean
# -------------------------
a_ar <- -2
b_max_ar <- 2
b_ar <- seq(a_ar, b_max_ar, length.out = 500)
phi <- -0.8

lambda_ar <- 0.5 * phi * (b_ar^2 - a_ar^2)

plot(b_ar, lambda_ar, type="l", lwd=2, col="red",
     xlab="b", ylab=expression(Lambda(b)),
     main=paste0("AR(1) Cumulative Lambda (a=", a_ar, ", b_max=", b_max_ar, ")"))
abline(h=0, lty=2, col="gray")
abline(v=0, lty=2, col="gray")

# -------------------------
# SETAR cumulative conditional mean (larger area)
# -------------------------
a_setar_large <- -20
b_max_setar_large <- 20
b_setar_large <- seq(a_setar_large, b_max_setar_large, length.out = 1000)

lambda_setar_large <- ifelse(b_setar_large < 0,
                             3*(b_setar_large - a_setar_large) + 0.4*(b_setar_large^2 - a_setar_large^2),
                             ifelse(b_setar_large == 0,
                                    3*(0 - a_setar_large) + 0.4*(0^2 - a_setar_large^2),
                                    (3*(0 - a_setar_large) + 0.4*(0^2 - a_setar_large^2)) +
                                      (-3*(b_setar_large - 0) - 0.4*(b_setar_large^2 - 0^2))
                             )
)

plot(b_setar_large, lambda_setar_large, type="l", lwd=2, col="blue",
     xlab="b", ylab=expression(Lambda(b)),
     main=paste0("SETAR Cumulative Lambda (a=", a_setar_large, ", b_max=", b_max_setar_large, ")"))
abline(h=0, lty=2, col="gray")
abline(v=0, lty=2, col="gray")

# -------------------------
# AR(1) cumulative conditional mean (larger area)
# -------------------------
a_ar_large <- -20
b_max_ar_large <- 20
b_ar_large <- seq(a_ar_large, b_max_ar_large, length.out = 1000)

lambda_ar_large <- 0.5 * phi * (b_ar_large^2 - a_ar_large^2)

plot(b_ar_large, lambda_ar_large, type="l", lwd=2, col="red",
     xlab="b", ylab=expression(Lambda(b)),
     main=paste0("AR(1) Cumulative Lambda (a=", a_ar_large, ", b_max=", b_max_ar_large, ")"))
abline(h=0, lty=2, col="gray")
abline(v=0, lty=2, col="gray")



```



### Estimated cumulative conditional mean
```{r}
set.seed(7)

# -------------------------
# 1️⃣ Simulate SETAR(2,1,1)
# -------------------------
simulate_setar <- function(n){
  r <- rnorm(n, sd = 1)
  x <- rep(NA, n)
  x[1] <- r[1]
  for(t in 2:n){
    if(x[t-1] < 0){
      x[t] <- 3 + 0.8 * x[t-1] + r[t]
    } else {
      x[t] <- -3 - 0.8 * x[t-1] + r[t]
    }
  }
  return(x)
}

# -------------------------
# 2️⃣ Cumulative means function
# -------------------------
cumulative_mean <- function(x, n.bin = 21){
  a <- min(x)
  b_max <- max(x)
  breaks <- seq(a, b_max, length.out = n.bin + 1)
  h <- diff(breaks)[1]
  
  # Cut into bins
  L <- split(x[-1], cut(x[-length(x)], breaks))
  if(!all(sapply(L, length) >= 5)) { 
    warning('Some bins have less than 5 points'); 
  }
  
  lambda <- gamma <- f.hat <- h.hat <- numeric(n.bin)
  
  for(i in 1:n.bin){
    x.bin <- L[[i]]
    lambda[i] <- mean(x.bin)
    f.hat[i] <- length(x.bin) / (n.bin * h)
    gamma[i] <- sum((x.bin - lambda[i])^2) / length(x.bin)
  }
  
  Lambda <- cumsum(lambda * h)
  for(i in 1:n.bin) h.hat[i] <- gamma[i] / f.hat[i]
  H.hat <- cumsum(h.hat * h)
  H.hat.b <- H.hat[n.bin]
  c.alpha <- 1.273
  Lambda.lower <- Lambda - c.alpha * n.bin^(-0.5) * sqrt(H.hat.b) * (1 + H.hat/H.hat.b)
  Lambda.upper <- Lambda + c.alpha * n.bin^(-0.5) * sqrt(H.hat.b) * (1 + H.hat/H.hat.b)
  
  # Bin centers
  b <- head(breaks, -1) + h/2
  list(b = b, Lambda = Lambda, Lambda.lower = Lambda.lower, Lambda.upper = Lambda.upper, a = a)
}

# -------------------------
# 3️⃣ Theoretical cumulative mean (3-piece integral)
# -------------------------
theoretical_lambda <- function(b, a){
  Lambda_theor <- numeric(length(b))
  for(i in seq_along(b)){
    bi <- b[i]
    if(bi < 0){
      Lambda_theor[i] <- 3*(bi - a) + 0.4*(bi^2 - a^2)
    } else if(a < 0 & bi >= 0){
      Lambda_theor[i] <- 3*(0 - a) + 0.4*(0^2 - a^2) + (-3*(bi - 0) - 0.4*(bi^2 - 0^2))
    } else {
      Lambda_theor[i] <- -3*(bi - a) - 0.4*(bi^2 - a^2)
    }
  }
  return(Lambda_theor)
}

# -------------------------
# 4️⃣ Explore different sample sizes
# -------------------------
n_list <- c(1000, 5000, 10000)
colors <- c("red","blue","green")

# Determine overall plotting range using large sample
x_tmp <- simulate_setar(max(n_list))
res_tmp <- cumulative_mean(x_tmp)
lambda_theor_tmp <- theoretical_lambda(res_tmp$b, res_tmp$a)
ylim_all <- range(c(lambda_theor_tmp))

# Plot setup
plot(NA, xlim = range(res_tmp$b), ylim = ylim_all,
     xlab = expression(X[t-1]), ylab = "Cumulative mean", type = "n",
     main = "Cumulative Means vs Theoretical Curve")

# Overlay theoretical cumulative mean
lines(res_tmp$b, theoretical_lambda(res_tmp$b, res_tmp$a), col = "black", lwd = 2, lty = 2)

# Empirical curves for each sample size
for(i in seq_along(n_list)){
  x <- simulate_setar(n_list[i])
  res <- cumulative_mean(x)
  lines(res$b, res$Lambda, col = colors[i], lwd = 2)
  lines(res$b, res$Lambda.lower, col = colors[i], lty = 2)
  lines(res$b, res$Lambda.upper, col = colors[i], lty = 2)
}

# Legend
legend("topright", legend = c(paste("n =", n_list), "Theoretical"),
       col = c(colors, "black"), lwd = 2, lty = c(rep(1, length(n_list)), 2), bty = "n")


```

```{r}
## Compare with AR(1) using cumulative means
n <- 1000
phi <- -0.8
x <- numeric(n)
x[1] <- rnorm(1)
for(t in 2:n) x[t] <- phi * x[t-1] + rnorm(1)

##----------------------------------------------------------------
## Parameters for histogram regression
n.bin <- 10
breaks <- seq(min(x), max(x), len = n.bin + 1)
h <- diff(breaks)[1]
lambda <- gamma <- f.hat <- h.hat <- numeric(n.bin)
##----------------------------------------------------------------

##----------------------------------------------------------------
## Cut into intervals conditioned on x_{t-1}
L <- split(x[-1], cut(x[-length(x)], breaks))

## Check that all bins have at least 5 points
if(!all(sapply(L, length) >= 5)) { 
  stop('Stopped: There are less than 5 points in one of the intervals')
}

## Calc histogram regressogram
for(i in 1:n.bin) {
  x.bin <- L[[i]]
  lambda[i] <- mean(x.bin)
  f.hat[i] <- length(x.bin) / (n.bin * h)
  gamma[i] <- sum((x.bin - lambda[i])^2) / length(x.bin)
}

## Cumulative function & confidence bands
c.alpha <- 1.273
Lambda <- cumsum(lambda * h)
for(i in 1:n.bin) h.hat[i] <- gamma[i] / f.hat[i]
H.hat <- cumsum(h.hat * h)
H.hat.b <- H.hat[n.bin]
Lambda.lower <- Lambda - c.alpha * n.bin^(-0.5) * sqrt(H.hat.b) * (1 + H.hat/H.hat.b)
Lambda.upper <- Lambda + c.alpha * n.bin^(-0.5) * sqrt(H.hat.b) * (1 + H.hat/H.hat.b)
##----------------------------------------------------------------

## Parametric Lambda for AR(1) using cumulative means
b <- head(breaks, -1) + h/2  # bin centers
lambda_cond <- phi * b        # AR(1) conditional mean
Lambda_param <- cumsum(lambda_cond * h)  # cumulative

## Safe y-range for plotting
ylim_all <- range(c(Lambda_param, Lambda, Lambda.lower, Lambda.upper), na.rm = TRUE)

## Empty plot frame
plot(NA, xlim = range(b), ylim = ylim_all,
     xlab = "b / index", ylab = expression(Lambda),
     type = "n")

## Plot parametric cumulative Lambda
lines(b, Lambda_param, col = "blue", lwd = 2)
abline(h = 0, lty = 2, col = "gray")

## Plot empirical cumulative Lambda with confidence bands
lines(b, Lambda, col = "black", lwd = 2)
lines(b, Lambda.lower, col = "red", lty = 2)
lines(b, Lambda.upper, col = "red", lty = 2)



```

## Part 4

### Loading data

```{r}
library(readr)
DataPart4 <- readr::read_csv("../Data/DataPart4.csv")
```

```{r}

Ph <- DataPart4$Ph
Ti <- DataPart4$Ti
Te <- DataPart4$Te
W <- DataPart4$W

U <- Ph/(Ti-Te)
DataPart4$U <- U
# Plotting W against U
# There is outlier with negative heat loss coefficient, comment and remove since it should be non-zero (double check)
plot(W,U, ylim = c(100, 300))

# Fitting local polynomial regression
fit <- loess(U ~ W, data = DataPart4, span = 0.7, degree = 2)

v <- seq(0, 10, length.out = 1000)
U_prd <- predict(fit, v)
plot(v, U_prd, type='l')

# DO RESIDUAL ANALYSIS

plot(fit$residuals)
qqnorm(fit$residuals)
qqline(fit$residuals)
```
## Part 5

### Loading data

```{r}
DataPart5 <- readr::read_csv("../Data/DataPart5.csv")
```

### Modelling ARMA

```{r}

x <- DataPart5$x  
x <- x-mean(x)
plot(x, type = "l", main = "X")
acf(x, lag.max = 40, main = "ACF of series")
pacf(x, lag.max = 40, main = "PACF of series")


```

### Adding p = 1 on t-2 based on PACF

```{r}
fit <- arima(x, order = c(2, 0, 0),
             include.mean = FALSE,
             fixed = c(0, NA),          # AR1 = 0, AR2 = estimate
             transform.pars = FALSE)

res <- residuals(fit)

acf(res, lag.max = 40, main = "ACF of residuals")
pacf(res, lag.max = 40, main = "PACF of residuals")

# 95% reference band for ACF/PACF (rough):
n <- length(res)
ci <- qnorm(0.975)/sqrt(n)
abline(h = c(-ci, ci), lty = 2)  # add after each plot if you like

# Ljung–Box test: want a large p-value (no remaining autocorrelation)
Box.test(res, lag = 24, type = "Ljung", fitdf = 2)
```

### LDF

```{r}
leaveOneOut <- function(D,plotIt=FALSE)
  {
    ## Find the bandwidth giving the best balance between bias and variance
    ## of the model by leave one out.

    ## Plot the data
    if(plotIt)
      {
        dev.new()
        par(mfrow=c(1,2))
        plot(D$xk, D$x)
      }
    ## Make the vector of bandwidths which are fitted
    span <- c(seq(0.2, 1, by=0.1),2,4,10)
    ## Matrix for keeping the residuals
    R <- matrix(nrow=nrow(D), ncol=length(span))
    ## Do it for each bandwidth
    for(ii in 1:length(span))
      {
        print(paste("  Fitting for bandwidth",ii,"of",length(span)))
        ## Do the local 2-order polynomial regression one time for each point
        ## leaving the point out while fitting, and then predicting the point.
        for(i in 1:nrow(D))
          {
            R[i,ii] <- D[i,"x"] - predict(loess(x ~ xk, dat=D[-i,], span=span[ii]), D[i,])
          }
      }
    ## Find the best bandwidth
    RSSkAll <- apply(R, 2, function(x){sum(x^2,na.rm=TRUE)})
    ## Take the RRS for the best span value
    spanBest <- span[which.min(RSSkAll)]
    ## Calculate the RSSk
    RSSk <- sum((D$x - predict(loess(x ~ xk, dat=D, span=spanBest), D))^2,na.rm=TRUE)
    ## Plot the fitted function
    if(plotIt)
      {
        DT <- D[order(D$xk),]
        lines(DT$xk, predict(loess(x ~ xk, dat=D, span=spanBest), DT), col=2)
        ## Plot RSSk as a function of the bandwidth
        plot(span, RSSkAll)
      }
    ##
    return(RSSk)
  }

## Make a simple ldf(x,lags)
ldf <- function(x,lags,nBoot=30,plotIt=TRUE,plotFits=FALSE)
{  
  ## Calculate Lag Dependence Functions
  ## by local 2-order polynomial regression with the loess() function,
  ## and leave one out to find the best bandwidth. Finally an approximate
  ## 95% confidence interval is calculated with simple bootstrapping.
  ## Input:
  ## x, is the time series to be analysed
  ## lags, are the values for which the LDF is calculated
  ## nBoot, is the number of bootstrapping samples to make
  ## plotIt, should the ldf be plotted
  ## plotFits, should the smoothed be plotted

  ## The result is kept in val
  val <- vector()
  ##
  for(i in 1:length(lags))
    {
      ## Take the k
      k <- lags[i]
      ## print text
      print(paste("Calculating ldf no. ",i," of ",length(lags), sep=""))
      ## Dataframe for modelling: xk is lagged k steps
      D <- data.frame(x=x[-(1:k)],xk=x[-((length(x)-k+1):length(x))])
      ## Leave one out optimization of the bandwidth with loess
      RSSk <- leaveOneOut(D,plotFits)
      ## Calculate the ldf
      RSS <- sum((D$x - mean(D$x))^2)
      val[i] <- (RSS - RSSk) / RSS
    }      
  
  ## Very simple bootstrapping
  iidVal <- vector()
  for(i in 1:nBoot)
    {
      ## Print to entertain the modeller ;-)
      print(paste("Calculating bootstrap no. ",i," of ",nBoot, sep=""))
      ## Bootstrapping to make a confidence band
      xr <- sample(x, min(length(x),100) ,replace=TRUE)
      ## Dataframe for modelling
      DR <- data.frame(x=xr[-1],xk=xr[-length(xr)])
      RSSk <- leaveOneOut(DR)
      ## The ldf is then calculated
      RSS <- sum((DR$x - mean(DR$x))^2)
      (iidVal[i] <- (RSS - RSSk) / RSS)
    }

  ## Plot the ldf
  if(plotIt)
    {
      dev.new()
      plot(c(0,lags), c(1,val), type="n", ylim=c(-1,1), ylab="LDF", main="Lag Dependence Functions", xaxt="n", xlab="lag")
      axis(1,c(0,lags))
      abline(0,0,col="gray")
      lines(c(0,lags), c(1,val), type="h")
      ## Draw the approximate 95% confidence interval
      abline(h=quantile(iidVal,0.95), col="blue", lty=2)
    }
  return(val)
}
```

### Running function up to 10 lags

```{r}
val <- ldf(res, 1:10)

# LDF significant at lag 2!!!
```

```{r}
n <- length(res)

plot(res[1:(n-2)], res[3:n], main = "e[t] Against e[t-2]") # Plotting residuals

plot(x[1:(n-2)], x[3:n], main = "X[t] Against X[t-2]") # Plotting x against x_{n-2}
# We see possibly 4 different regimes! SETAR!!!
# Note that the regimes have different variances

```

### Dividing data based on regions

```{r}
cut_offs <- c(-4.2, -0.235, 2.77)
x1 <- rep(NA, n)
x2 <- rep(NA, n)
x3 <- rep(NA, n)
x4 <- rep(NA, n)
# For loop for splitting data into 4 regions
for(t in 3:n){
  if(x[t-2] < cut_offs[1]){
    x1[t] <- x[t] 
  }
  if(x[t-2] >= cut_offs[1] && x[t-2] < cut_offs[2]){
    x2[t] <- x[t]
  }
  if(x[t-2] >= cut_offs[2] && x[t-2] < cut_offs[3]){
    x3[t] <- x[t]
  }
  if(x[t-2] >= cut_offs[3]){
    x4[t] <- x[t]
  }
}

```

```{r}
n <- length(x)
lag2 <- x[1:(n-2)]   # x_{t-2}
cur  <- x[3:n]       # x_t

plot(lag2, cur, pch = 16, col = "grey80",
     xlab = "X[t-2]", ylab = "X[t]", main = "X[t] against X[t-2] by Region")

cols <- c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728")  # x1..x4

# indices where each group is present (your x1..x4 hold x_t at time t)
i1 <- which(!is.na(x1)); points(x[i1-2], x[i1], pch=16, col=cols[1])
i2 <- which(!is.na(x2)); points(x[i2-2], x[i2], pch=16, col=cols[2])
i3 <- which(!is.na(x3)); points(x[i3-2], x[i3], pch=16, col=cols[3])
i4 <- which(!is.na(x4)); points(x[i4-2], x[i4], pch=16, col=cols[4])

labs <- list(
  bquote(x[t-2] %in% "(" * -infinity * "," ~ .(cut_offs[1]) * ")"),
  bquote(x[t-2] %in% "[" * .(cut_offs[1]) * "," ~ .(cut_offs[2]) * ")"),
  bquote(x[t-2] %in% "[" * .(cut_offs[2]) * "," ~ .(cut_offs[3]) * ")"),
  bquote(x[t-2] %in% "[" * .(cut_offs[3]) * "," ~ infinity * ")")
)

legend("bottomleft", legend = labs, col = cols, pch = 16, bty = "n", cex = 0.9)
```


```{r}
# cutoffs
cut_offs <- c(-4.2, -0.235, 2.77)

# align x_t and x_{t-2}
n    <- length(x)
lag2 <- x[1:(n-2)]
cur  <- x[3:n]

# regime by x_{t-2}
grp <- cut(lag2, breaks = c(-Inf, cut_offs, Inf), right = FALSE,
           labels = c("R1","R2","R3","R4"))

df <- data.frame(x_t = cur, x_lag2 = lag2, grp)

# --- Option A: one model with group-specific intercepts & slopes
m <- lm(x_t ~ 0 + grp + grp:x_lag2, data = df)  # 0 removes global intercept
coef(m)                                         # prints α_j and β_j (per group)

# Tidy table of coefficients
get_ab <- function(fit) {
  co <- coef(fit)
  data.frame(
    Regime = levels(grp),
    alpha  = unname(co[paste0("grp", levels(grp))]),
    beta   = unname(co[paste0("grp", levels(grp), ":x_lag2")])
  )
}
ab <- get_ab(m); ab
```

```{r}
plot(df$x_lag2, df$x_t, pch=16, col=cols[df$grp],
     xlab="x[t-2]", ylab="x[t]", main="Fitted Lines in Each Region")

for (r in levels(df$grp)) {
  d <- df[df$grp == r, ]
  a <- ab$alpha[ab$Regime == r]
  b <- ab$beta [ab$Regime == r]
  xx <- range(d$x_lag2, na.rm = TRUE)
  lines(xx, a + b*xx, lwd = 2)
}

labs <- list(
  bquote(x[t-2] %in% "(" * -infinity * "," ~ .(cut_offs[1]) * ")"),
  bquote(x[t-2] %in% "[" * .(cut_offs[1]) * "," ~ .(cut_offs[2]) * ")"),
  bquote(x[t-2] %in% "[" * .(cut_offs[2]) * "," ~ .(cut_offs[3]) * ")"),
  bquote(x[t-2] %in% "[" * .(cut_offs[3]) * "," ~ infinity * ")")
)

legend("bottomleft", legend = labs, col = cols, pch = 16, bty = "n", cex = 0.9)



```
